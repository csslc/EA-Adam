import collections.abc
import math
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.nn import init as init
import torchvision
import warnings
from itertools import repeat
from einops import rearrange

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
            'The distribution of values may be incorrect.',
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        low = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [low, up], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * low - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution.

    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py

    The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

# From PyTorch
def _ntuple(n):

    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))

    return parse

class ResidualBlockNoBNDynamic(nn.Module):
    """Residual block without BN.
    It has a style of:
        ---Conv-ReLU-Conv-+-
         |________________|
    Args:
        num_feat (int): Channel number of intermediate features.
            Default: 64.
        res_scale (float): Residual scale. Default: 1.
        pytorch_init (bool): If set to True, use pytorch default init,
            otherwise, use default_init_weights. Default: False.
    """

    def __init__(self, args, num_feat=64, res_scale=1, num_models=5):
        super(ResidualBlockNoBNDynamic, self).__init__()
        self.res_scale = res_scale
        self.conv1 = Dynamic_conv2d(args, num_feat, num_feat, 3, groups=1, if_bias=True, K=num_models)
        self.conv2 = Dynamic_conv2d(args, num_feat, num_feat, 3, groups=1, if_bias=True, K=num_models)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, inputs):
        weights = inputs[1]
        x = inputs[0]
        identity = x.clone()
        out, weight = self.conv1(x)
        weight = weight.detach().cpu()
        weights.append(weight)
        out = self.relu(out)
        conv2_input = out
        out, weight = self.conv2(conv2_input)
        weight = weight.detach().cpu()
        weights.append(weight)
        out = identity + out * self.res_scale
        outputs = [out, weights]
        return outputs

def make_layer(basic_block, num_basic_block, **kwarg):
    """Make layers by stacking the same blocks.
    Args:
        basic_block (nn.module): nn.module class for basic block.
        num_basic_block (int): number of blocks.
    Returns:
        nn.Sequential: Stacked blocks in nn.Sequential.
    """
    layers = []
    for _ in range(num_basic_block):
        layers.append(basic_block(**kwarg))
    return nn.Sequential(*layers)

class Fusion(nn.Module):
    def __init__(self, args, in_planes):
        super(Fusion, self).__init__()

        in_nc = in_planes
        nf = args.num_feat_fusion
        num_params = args.num_params
        num_networks = args.num_network
        use_bias = args.use_bias

        self.ConvNet = nn.Sequential(*[
            nn.Conv2d(in_nc, nf, kernel_size=5, stride=1, padding=2),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(nf, nf, kernel_size=5, stride=1, padding=2, bias=use_bias),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(nf, num_params, kernel_size=5, stride=1, padding=2, bias=use_bias),
            nn.LeakyReLU(0.2, True),
        ])

        self.globalPooling = nn.AdaptiveAvgPool2d((1, 1))

        self.MappingNet = nn.Sequential(*[
            nn.Linear(num_params, 15),
            nn.LeakyReLU(0.2, True),
            nn.Linear(15, num_networks),
            nn.Sigmoid()
        ])

    def forward(self, input):
        conv = self.ConvNet(input)
        flat = self.globalPooling(conv)
        out_params = flat.view(flat.size()[:2])
        mapped_weights = self.MappingNet(out_params)

        return out_params, mapped_weights

class Dynamic_conv2d(nn.Module):
    def __init__(self, args, in_planes, out_planes, kernel_size, stride=1, padding=1, dilation=1, groups=1, if_bias=True, K=5, init_weight=False):
        super(Dynamic_conv2d, self).__init__()
        assert in_planes % groups == 0
        self.in_planes = in_planes
        self.out_planes = out_planes
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        self.if_bias = if_bias
        self.fusion = Fusion(args, in_planes)
        self.K = K

        self.weight = nn.Parameter(torch.randn(K, out_planes, in_planes//groups, kernel_size, kernel_size), requires_grad=args.is_train_expert)
        if self.if_bias:
            self.bias = nn.Parameter(torch.Tensor(K, out_planes), requires_grad=args.is_train_expert)
        else:
            self.bias = None
        if init_weight:
            self._initialize_weights()

    def _initialize_weights(self):
        for i in range(self.K):
            nn.init.kaiming_uniform_(self.weight[i])
            if self.if_bias:
                nn.init.constant_(self.bias[i], 0)

    def forward(self, x):
        _, softmax_attention = self.fusion(x)
        softmax_attention = softmax_attention/(torch.sum(softmax_attention, 1).unsqueeze(1).repeat([1,5])+0.0001)
        # softmax_attention = torch.tensor([[0.2180, 0.2018, 0.1942, 0.2023, 0.1837]]).cuda()
        # softmax_attention = 0.2 * torch.ones(softmax_attention.shape).cuda()
        batch_size, in_planes, height, width = x.size()
        x = x.contiguous().view(1, -1, height, width)
        weight = self.weight.view(self.K, -1)

        aggregate_weight = torch.mm(softmax_attention, weight).view(-1, self.in_planes, self.kernel_size, self.kernel_size)
        if self.bias is not None:
            aggregate_bias = torch.mm(softmax_attention, self.bias).view(-1)
            output = F.conv2d(x, weight=aggregate_weight, bias=aggregate_bias, stride=self.stride, padding=self.padding,
                              dilation=self.dilation, groups=self.groups*batch_size)
        else:
            output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,
                              dilation=self.dilation, groups=self.groups * batch_size)

        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))             
        return output, softmax_attention

class Dynamic_norm_layer(nn.Module):
    def __init__(self, args, in_planes, num_planes, groups=1, if_bias=True, K=5, init_weight=False):
        super(Dynamic_norm_layer, self).__init__()
        self.in_planes = in_planes
        self.num_planes = num_planes
        self.groups = groups
        self.if_bias = if_bias
        self.fusion = Fusion(args, in_planes)
        self.K = K

        self.weight = nn.Parameter(torch.randn(K, num_planes), requires_grad=args.is_train_expert)
        if self.if_bias:
            self.bias = nn.Parameter(torch.Tensor(K, num_planes), requires_grad=args.is_train_expert)
        else:
            self.bias = None
        if init_weight:
            self._initialize_weights()

    def _initialize_weights(self):
        for i in range(self.K):
            nn.init.kaiming_uniform_(self.weight[i])
            if self.if_bias:
                nn.init.constant_(self.bias[i], 0)

    def forward(self, x, x_size):
        x_forfs = rearrange(x, 'b (h w) c -> b c h w', h=x_size[0])
        _, softmax_attention = self.fusion(x_forfs)
        softmax_attention = softmax_attention/(torch.sum(softmax_attention, 1).unsqueeze(1).repeat([1,5])+0.0001)
        # softmax_attention = torch.tensor([[0.2180, 0.2018, 0.1942, 0.2023, 0.1837]]).cuda()
        # softmax_attention = 0.2 * torch.ones(softmax_attention.shape).cuda()
        # batch_size, hw, in_planes = x.size()
        # x = x.contiguous().view(1, -1, in_planes)
        weight = self.weight.view(self.K, -1)

        aggregate_weight = torch.mm(softmax_attention, weight).view(-1)
        aggregate_weight = rearrange(aggregate_weight, '(b c) -> b c', b=x_forfs.shape[0])
        if self.bias is not None:
            aggregate_bias = torch.mm(softmax_attention, self.bias).view(-1)
            aggregate_bias = rearrange(aggregate_bias, '(b c) -> b c', b=x_forfs.shape[0])
            for i in range(x.shape[0]):
                # weight_sub = self.weight[...,i*self.in_planes:(i+1)*self.in_planes]
                # bias_sub = self.bias[...,i*self.in_planes:(i+1)*self.in_planes]
                # for j in range(self.K):
                #     weight = softmax_attention[j][i]*weight_sub[j] + 
                weight_sub = aggregate_weight[i]
                bias_sub = aggregate_bias[i]
                output = F.layer_norm(x[i], (self.in_planes,), weight=weight_sub, bias=bias_sub)
                if i == 0:
                    output_final = output.unsqueeze(0)   
                else:
                    output_final = torch.cat((output_final, output.unsqueeze(0)), 0)
        else:
            print('error in Dynamic_norm_layer!')
            # output = F.layer_norm(x, self.in_planes, weight=aggregate_weight, bias=None)

        # output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))
        return output_final, softmax_attention


class Dynamic_qkv_linear(nn.Module):
    def __init__(self, args, in_planes, out_planes, if_bias=True, K=5, init_weight=False):
        super(Dynamic_qkv_linear, self).__init__()
        self.in_planes = in_planes
        self.out_planes = out_planes
        self.if_bias = if_bias
        self.fusion = Fusion(args, in_planes)
        self.K = K
        self.b = args.batch_size
        self.w = args.window_size

        self.weight = nn.Parameter(torch.randn(K, out_planes, in_planes), requires_grad=args.is_train_expert)
        if self.if_bias:
            self.bias = nn.Parameter(torch.Tensor(K, out_planes), requires_grad=args.is_train_expert)
        else:
            self.bias = None
        if init_weight:
            self._initialize_weights()

    def _initialize_weights(self):
        for i in range(self.K):
            nn.init.kaiming_uniform_(self.weight[i])
            if self.if_bias:
                nn.init.constant_(self.bias[i], 0)

    def forward(self, x, x_size):
        x_forfs = rearrange(x, '(b nw1 nw2) (ws1 ws2) c -> b c (nw1 ws1) (nw2 ws2)',nw2 = x_size[1]//self.w, ws1=self.w, nw1 =x_size[0]//self.w)
        _, softmax_attention = self.fusion(x_forfs)
        softmax_attention = softmax_attention/(torch.sum(softmax_attention, 1).unsqueeze(1).repeat([1,5])+0.0001)
        # softmax_attention = torch.tensor([[0.2180, 0.2018, 0.1942, 0.2023, 0.1837]]).cuda()
        # softmax_attention = 0.2 * torch.ones(softmax_attention.shape).cuda()
        # batch_size, in_planes, height, width = x.size()
        # x = x.contiguous().view(1, -1, height, width)
        weight = self.weight.view(self.K, -1)
        
        aggregate_weight = torch.mm(softmax_attention, weight).view(-1, self.in_planes)
        x_sub = rearrange(x, '(b nw1 nw2) ws c -> b (nw1 nw2) ws c', nw2 = x_size[1]//self.w, nw1 =x_size[0]//self.w)
        b = x_sub.shape[0]
        aggregate_weight = rearrange(aggregate_weight, '(b c) in -> b c in', b=b)
        if self.bias is not None:
            aggregate_bias = torch.mm(softmax_attention, self.bias).view(-1)
            aggregate_bias = rearrange(aggregate_bias, '(b c)-> b c', b=b)
            for i in range(b):
                weight_sub = aggregate_weight[i]
                bias_sub = aggregate_bias[i]
                output = F.linear(x_sub[i].unsqueeze(0), weight=weight_sub, bias=bias_sub)
                if i == 0:
                    output_final = output   
                else:
                    output_final = torch.cat((output_final, output), 0)
            # output = F.linear(x, weight=aggregate_weight, bias=aggregate_bias)
        else:
            print('error in Dynamic_linear!')
            # output = F.linear(x, weight=aggregate_weight, bias=None)
        output_final = rearrange(output_final, 'b (nw1 nw2) ws c -> (b nw1 nw2) ws c', nw2 = x_size[1]//self.w, nw1 =x_size[0]//self.w)
        # output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))
        return output_final, softmax_attention

class Dynamic_linear(nn.Module):
    def __init__(self, args, in_planes, out_planes, if_bias=True, K=5, init_weight=False):
        super(Dynamic_linear, self).__init__()
        self.in_planes = in_planes
        self.out_planes = out_planes
        self.if_bias = if_bias
        self.fusion = Fusion(args, in_planes)
        self.K = K

        self.weight = nn.Parameter(torch.randn(K, out_planes, in_planes), requires_grad=args.is_train_expert)
        if self.if_bias:
            self.bias = nn.Parameter(torch.Tensor(K, out_planes), requires_grad=args.is_train_expert)
        else:
            self.bias = None
        if init_weight:
            self._initialize_weights()

    def _initialize_weights(self):
        for i in range(self.K):
            nn.init.kaiming_uniform_(self.weight[i])
            if self.if_bias:
                nn.init.constant_(self.bias[i], 0)

    def forward(self, x, x_size):
        x_forfs = rearrange(x, 'b (h w) c -> b c h w', h=x_size[0])
        _, softmax_attention = self.fusion(x_forfs)
        softmax_attention = softmax_attention/(torch.sum(softmax_attention, 1).unsqueeze(1).repeat([1,5])+0.0001)
        # softmax_attention = torch.tensor([[0.2180, 0.2018, 0.1942, 0.2023, 0.1837]]).cuda()
        # softmax_attention = 0.2 * torch.ones(softmax_attention.shape).cuda()
        # batch_size, in_planes, height, width = x.size()
        # x = x.contiguous().view(1, -1, height, width)
        weight = self.weight.view(self.K, -1)

        aggregate_weight = torch.mm(softmax_attention, weight).view(-1, self.in_planes)
        aggregate_weight = rearrange(aggregate_weight, '(b c) in -> b c in', b=x_forfs.shape[0])
        if self.bias is not None:
            aggregate_bias = torch.mm(softmax_attention, self.bias).view(-1)
            aggregate_bias = rearrange(aggregate_bias, '(b c)-> b c', b=x_forfs.shape[0])
            for i in range(x_forfs.shape[0]):
                weight_sub = aggregate_weight[i]
                bias_sub = aggregate_bias[i]
                output = F.linear(x[i].unsqueeze(0), weight=weight_sub, bias=bias_sub)
                if i == 0:
                    output_final = output  
                else:
                    output_final = torch.cat((output_final, output), 0)
            # output = F.linear(x, weight=aggregate_weight, bias=aggregate_bias)
        else:
            print('error in Dynamic_linear!')
            # output = F.linear(x, weight=aggregate_weight, bias=None)

        # output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))
        return output_final, softmax_attention

to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple